@article{Cour2008,
abstract = {Compressive sensing (CS) is an emerging field that provides a frame- work for image recovery using sub-Nyquist sampling rates. The CS theory shows that a signal can be reconstructed from a small set of random projections, pro- vided that the signal is sparse in some basis, e.g., wavelets. In this paper, we describe a method to directly recover background subtracted images using CS and discuss its applications in some communication constrained multi-camera computer vision problems. We show how to apply the CS theory to recover ob- ject silhouettes (binary background subtracted images) when the objects of in- terest occupy a small portion of the camera view, i.e., when they are sparse in the spatial domain. We cast the background subtraction as a sparse approxima- tion problem and provide different solutions based on convex optimization and total variation. In our method, as opposed to learning the background, we learn and adapt a low dimensional compressed representation of it, which is sufficient to determine spatial innovations; object silhouettes are then estimated directly using the compressive samples without any auxiliary image reconstruction. We also discuss simultaneous appearance recovery of the objects using compressive measurements. In this case, we show that it may be necessary to reconstruct one auxiliary image. To demonstrate the performance of the proposed algorithm, we provide results on data captured using a compressive single-pixel camera. We also illustrate that our approach is suitable for image coding in communication constrained problems by using data captured bymultiple conventional cameras to provide 2D tracking and 3D shape reconstruction results with compressive mea- surements.},
archivePrefix = {arXiv},
arxivId = {1311.2901},
author = {Cour, Timothee and Jordan, Chris and Miltsakaki, Eleni and Taskar, Ben},
doi = {10.1007/978-3-540-88693-8},
eprint = {1311.2901},
file = {:home/franky/Documents/Shared Documents/MLDM/Project/Papers/fulltext.pdf:pdf},
isbn = {978-3-540-88692-1},
issn = {0302-9743},
number = {October},
pages = {158--171},
pmid = {16546397},
title = {{Computer Vision – ECCV 2008}},
url = {http://link.springer.com/10.1007/978-3-540-88693-8},
volume = {5305},
year = {2008}
}
@article{Everingham2006,
abstract = {We investigate the problem of automatically labelling appearances of characters in TV or film material. This is tremendously challenging due to the huge variation in imaged appearance of each character and the weakness and ambiguity of available annotation. However, we demonstrate that high precision can be achieved by combining multiple sources of information, both visual and textual. The principal novelties that we introduce are: (i) automatic generation of time stamped character annotation by aligning subtitles and transcripts; (ii) strengthening the supervisory information by identifying when characters are speaking; (iii) using complementary cues of face matching and clothing matching to propose common annotations for face tracks. Results are presented on episodes of the TV series "Buffy the Vampire Slayer".},
author = {Everingham, M. R. and Sivic, Josef and Zisserman, Andrew},
doi = {10.5244/C.20.92},
file = {:home/franky/Documents/Shared Documents/MLDM/Project/Papers/everingham06a.pdf:pdf},
isbn = {1-901725-32-4},
journal = {Proceedings of the British Machine Vision Conference 2006},
keywords = {Machine Vision},
pages = {92.1--92.10},
title = {{Hello! My name is... Buffy'' -- Automatic Naming of Characters in TV Video}},
url = {http://eprints.pascal-network.org/archive/00002192/{\%}5Cnhttp://www.bmva.org/bmvc/2006/papers/340.html},
year = {2006}
}
@article{K2009,
abstract = {A standard solution for aligning scripts to movies is to use dynamic time warping with the subtitles (Everingham et al., BMVC 2006). We investigate the problem of aligning scripts to TV video/movies in cases where subtitles are not available, e.g. in the case of silent films or for film passages which are non-verbal. To this end we identify a number of "modes of alignment" and train classifiers for each of these. The modes include visual features, such as locations and face recognition, and audio features such as speech. In each case the feature gives some alignment information, but is too noisy when used independently. We show that combining the different features into a single cost function and optimizing this using dynamic programming, leads to a performance superior to each of the individual features. The method is assessed on episodes from the situation comedy Seinfeld, and on Charlie Chaplin and Indian movies.},
author = {K, Pramod Sankar and Jawahar, C V and Zisserman, Andrew},
doi = {10.5244/C.23.121},
file = {:home/franky/Documents/Shared Documents/MLDM/Project/Papers/Paper273.pdf:pdf},
isbn = {1-901725-39-1},
journal = {Movie},
keywords = {learning,machine vision,statistics {\&} optimisation},
pages = {1--11},
title = {{Subtitle-free Movie to Script Alignment}},
url = {http://eprints.pascal-network.org/archive/00006560/},
volume = {1},
year = {2009}
}
@misc{sabater_2017, title={Automatic Subtitle Synchronization – Machine Learnings}, url={https://machinelearnings.co/automatic-subtitle-synchronization-e188a9275617}, journal={Machine Learnings}, publisher={Machine Learnings}, author={Sabater, Alberto}, year={2017}, month={Sep}}